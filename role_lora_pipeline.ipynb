{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPO/blob/main/path/to/your_notebook.ipynb)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab_type": "code",
                "id": "install_packages"
            },
            "outputs": [],
            "source": [
                "!pip install -q transformers datasets peft accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "instructions"
            },
            "source": [
                "# Instructions\n",
                "To run this notebook in Colab: open it from GitHub/Drive in Colab, then click Runtime â–¸ Run all."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-da1e3bb7",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "BASE_MODEL = \"EleutherAI/gpt-neo-125M\"   # ~500 MB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-52909785",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "examples = [\n",
                "    {\n",
                "        \"role\": \"Detective\",\n",
                "        \"text\": \"Role: Detective\\nUser: Describe a crime scene.\\nAssistant: The alley smelled of rust and secrets, every shadow a possible clue.\"\n",
                "    },\n",
                "    {\n",
                "        \"role\": \"Poet\",\n",
                "        \"text\": \"Role: Poet\\nUser: Write about the sunrise.\\nAssistant: Dawn spills saffron light across the quiet roofs of the city.\"\n",
                "    },\n",
                "    {\n",
                "        \"role\": \"Comedian\",\n",
                "        \"text\": \"Role: Comedian\\nUser: Tell a joke about cats.\\nAssistant: Why did the cat join Instagram? To get more pawsitive feedback!\"\n",
                "    },\n",
                "]\n",
                "dataset = Dataset.from_list(examples)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-73925259",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "def tokenize_fn(example):\n",
                "    return tokenizer(example[\"text\"], truncation=True, max_length=512)\n",
                "\n",
                "tokenized = dataset.map(tokenize_fn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-7c62f5fb",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    load_in_8bit=True,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"c_attn\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-c9992139",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from transformers import TrainingArguments, Trainer\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"role-lora\",\n",
                "    per_device_train_batch_size=2,\n",
                "    gradient_accumulation_steps=8,\n",
                "    num_train_epochs=3,\n",
                "    learning_rate=2e-4,\n",
                "    fp16=True,\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized,\n",
                ")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-4db24293",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "model.save_pretrained(\"role-lora/adapter\")\n",
                "tokenizer.save_pretrained(\"role-lora/adapter\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "#VSC-34c75876",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from peft import PeftModel\n",
                "from transformers import AutoModelForCausalLM\n",
                "\n",
                "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")\n",
                "lora = PeftModel.from_pretrained(base, \"role-lora/adapter\")\n",
                "\n",
                "def generate(role, user_prompt):\n",
                "    full_prompt = f\"Role: {role}\\nUser: {user_prompt}\\nAssistant:\"\n",
                "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(lora.device)\n",
                "    out = lora.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=150,\n",
                "        do_sample=True,\n",
                "        temperature=0.8,\n",
                "        top_p=0.9\n",
                "    )\n",
                "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
                "\n",
                "generate(\"Poet\", \"Write about an autumn forest.\")\n",
                "generate(\"Detective\", \"Describe a mysterious stranger.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
