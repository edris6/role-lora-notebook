{"nbformat": 4, "nbformat_minor": 5, "metadata": {"language": "python"}, "cells": [{"id": "4086d0d0", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install -q transformers datasets peft accelerate bitsandbytes", "outputs": []}, {"id": "8eefa8fe", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "BASE_MODEL = \"EleutherAI/gpt-neo-125M\"   # ~500 MB", "outputs": []}, {"id": "8827376c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from datasets import Dataset\n\nexamples = [\n    {\n        \"role\": \"Detective\",\n        \"text\": \"Role: Detective\\nUser: Describe a crime scene.\\nAssistant: The alley smelled of rust and secrets, every shadow a possible clue.\"\n    },\n    {\n        \"role\": \"Poet\",\n        \"text\": \"Role: Poet\\nUser: Write about the sunrise.\\nAssistant: Dawn spills saffron light across the quiet roofs of the city.\"\n    },\n    {\n        \"role\": \"Comedian\",\n        \"text\": \"Role: Comedian\\nUser: Tell a joke about cats.\\nAssistant: Why did the cat join Instagram? To get more pawsitive feedback!\"\n    },\n]\ndataset = Dataset.from_list(examples)", "outputs": []}, {"id": "2f99b6cc", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_fn(example):\n    return tokenizer(example[\"text\"], truncation=True, max_length=512)\n\ntokenized = dataset.map(tokenize_fn)", "outputs": []}, {"id": "5a2bacc2", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from transformers import AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_8bit=True,\n    device_map=\"auto\"\n)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"c_attn\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)", "outputs": []}, {"id": "2baee663", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"role-lora\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=10,\n    save_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized,\n)\ntrainer.train()", "outputs": []}, {"id": "6303b5d3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "model.save_pretrained(\"role-lora/adapter\")\ntokenizer.save_pretrained(\"role-lora/adapter\")", "outputs": []}, {"id": "548087da", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from peft import PeftModel\nfrom transformers import AutoModelForCausalLM\n\nbase = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")\nlora = PeftModel.from_pretrained(base, \"role-lora/adapter\")\n\ndef generate(role, user_prompt):\n    full_prompt = f\"Role: {role}\\nUser: {user_prompt}\\nAssistant:\"\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(lora.device)\n    out = lora.generate(\n        **inputs,\n        max_new_tokens=150,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.9\n    )\n    print(tokenizer.decode(out[0], skip_special_tokens=True))\n\ngenerate(\"Poet\", \"Write about an autumn forest.\")\ngenerate(\"Detective\", \"Describe a mysterious stranger.\")", "outputs": []}]}