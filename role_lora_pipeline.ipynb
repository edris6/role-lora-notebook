{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Role LoRA Fine-Tuning Notebook\nThis notebook loads a base GPT-Neo model, applies LoRA adapters, tokenizes your dataset, and trains with the HuggingFace Trainer API."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Install dependencies ---\n!pip install -q transformers datasets peft bitsandbytes" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Imports ---\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\nfrom datasets import load_dataset" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Load dataset ---\ndataset = load_dataset('json', data_files={'train': 'train.json'})['train']\ntexts = dataset['text']" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Load tokenizer ---\nBASE_MODEL = 'EleutherAI/gpt-neo-1.3B'  # change to your model\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\ntokenizer.pad_token = tokenizer.eos_token" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Tokenize dataset with padding ---\ndef tokenize_function(examples):\n    return tokenizer(examples, padding='max_length', truncation=True, max_length=128)\n\ntokenized = dataset.map(lambda x: tokenize_function(x['text']), batched=True)" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Load base model and prepare for LoRA (8-bit) ---\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map='auto',\n    quantization_config=bnb_config\n)\n\n# Prepare model for int8 training (necessary for LoRA)\nmodel = prepare_model_for_int8_training(model)" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Configure LoRA ---\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=['q_proj','v_proj'],  # supported modules in GPTNeoAttention\n    lora_dropout=0.05,\n    bias='none',\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Data collator ---\ndata_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Training arguments ---\ntraining_args = TrainingArguments(\n    output_dir='role-lora',\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=10,\n    save_strategy='epoch',\n    report_to='none'  # disables WandB logging\n)" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Initialize Trainer ---\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized,\n    data_collator=data_collator\n)" 
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Train model ---\ntrainer.train()" 
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
